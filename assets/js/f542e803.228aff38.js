"use strict";(globalThis.webpackChunkgsf_docusaurus_template=globalThis.webpackChunkgsf_docusaurus_template||[]).push([[7546],{1559(e,i,n){n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"catalog/ai/compress-ml-models-for-inference","title":"Optimize the size of AI/ML models","description":"Large-scale AI/ML models require significant storage space and take more resources to run as compared to optimized models.","source":"@site/docs/catalog/ai/compress-ml-models-for-inference.md","sourceDirName":"catalog/ai","slug":"/catalog/ai/compress-ml-models-for-inference","permalink":"/catalog/ai/compress-ml-models-for-inference","draft":false,"unlisted":false,"editUrl":"https://github.com/Green-Software-Foundation/patterns/edit/main/docs/catalog/ai/compress-ml-models-for-inference.md","tags":[{"inline":true,"label":"ai","permalink":"/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/tags/machine-learning"},{"inline":true,"label":"role:data-scientist","permalink":"/tags/role-data-scientist"},{"inline":true,"label":"size:small","permalink":"/tags/size-small"}],"version":"current","frontMatter":{"version":1,"submitted_by":"navveenb","published_date":"2022-11-10T00:00:00.000Z","category":"ai","description":"Large-scale AI/ML models require significant storage space and take more resources to run as compared to optimized models.","tags":["ai","machine-learning","role:data-scientist","size:small"]}}');var t=n(4848),a=n(8453);const o={version:1,submitted_by:"navveenb",published_date:new Date("2022-11-10T00:00:00.000Z"),category:"ai",description:"Large-scale AI/ML models require significant storage space and take more resources to run as compared to optimized models.",tags:["ai","machine-learning","role:data-scientist","size:small"]},r="Optimize the size of AI/ML models",l={},c=[{value:"Description",id:"description",level:2},{value:"Solution",id:"solution",level:2},{value:"SCI Impact",id:"sci-impact",level:2},{value:"Assumptions",id:"assumptions",level:2},{value:"Considerations",id:"considerations",level:2},{value:"References",id:"references",level:2}];function d(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"optimize-the-size-of-aiml-models",children:"Optimize the size of AI/ML models"})}),"\n",(0,t.jsx)(i.h2,{id:"description",children:"Description"}),"\n",(0,t.jsx)(i.p,{children:"Large-scale AI/ML models require significant storage space and take more resources to run as compared to optimized models."}),"\n",(0,t.jsx)(i.h2,{id:"solution",children:"Solution"}),"\n",(0,t.jsx)(i.p,{children:"Optimizing the size of the AI/ML model can save on storage space and take up less memory. Apply strategies like quantization and evaluate the optimization changes against the desired accuracy."}),"\n",(0,t.jsx)(i.h2,{id:"sci-impact",children:"SCI Impact"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.code,{children:"SCI = (E * I) + M per R"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://grnsft.org/sci",children:"Software Carbon Intensity Spec"})}),"\n",(0,t.jsx)(i.p,{children:"Optimizing the AI/ML model impacts SCI as follows:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"E"}),": Having an optimized AI model would reduce the energy consumption for your AI/ML inference, save storage space and network bandwidth and consequently, the E number should decrease."]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"assumptions",children:"Assumptions"}),"\n",(0,t.jsx)(i.p,{children:"None"}),"\n",(0,t.jsx)(i.h2,{id:"considerations",children:"Considerations"}),"\n",(0,t.jsx)(i.p,{children:"None"}),"\n",(0,t.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://www.tensorflow.org/lite/performance/model_optimization",children:"Model optimization"})})]})}function m(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,i,n){n.d(i,{R:()=>o,x:()=>r});var s=n(6540);const t={},a=s.createContext(t);function o(e){const i=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:i},e.children)}}}]);